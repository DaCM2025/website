var relearn_searchindex = [
  {
    "breadcrumb": "Data and Code Management",
    "content": "Explore the nature of this course and understand its various prerequisites.",
    "description": "Explore the nature of this course and understand its various prerequisites.",
    "tags": [],
    "title": "Syllabus",
    "uri": "/en/syllabus/index.html"
  },
  {
    "breadcrumb": "Data and Code Management \u003e Tutorials",
    "content": "Installing and setting up Git, GitHub, and a Git GUI In this section we will install a distributed version-control system Git, register a new user at GitHub and connect them together.\n1. Installing and setting up Git Download Git installer from git website by clicking on “Download for Windows” or “Download for macOS” button, automatically detected for your machine. Open the downloaded file and follow the proposed steps Configure your Git to let it know who you are. Do it by opening a command prompt or Terminal window on your computer and set “Your Name” and “your_email@unil.ch{.email}” git config --global user.name \"Your Name\" git config --global user.email \"your_email@unil.ch\" Replace “Your Name” with your preferred name or the name you want to associate with your Git commits. Please also use your UNIL email address, so that we can exploit GitHub Student Developer Pack afterwards.\nCheck yourself: You can verify your Git configurations by running the following command:\ngit config --global --list It will display your configured username and email.\nNote: For Mac users, Git could be already preinstalled. However, Apple does not provide the latest version, that is why we have just installed the latest Git.\nNote: Sometimes RStudio has a wrong path to git command. To check it, go to Tools -\u003e Global Options… -\u003e Git/SVN, check the box “Enable version control interface for RStudio projects”. Then, “Git executable” and which git/where git (for Mac/Windows users, respectively) should be the same. Otherwise, copy the path from Terminal to RStudio.\nTo check if it worked, type which git in Terminal and expect to see /usr/local/bin/git.\n2. Registering a GitHub account Visit https://github.com and fill in the fields. Please use the same email address and short username without special symbols like hyphen, periods, etc. Furthermore, use free plan.\nSet up a GitHub Student Developer Pack by visiting https://education.github.com/.\nNote: The next step can be avoided (more convenient) by installing GitHub Desktop.\nNext, go to your GitHub account settings by clicking on your profile picture in the top-right corner of the GitHub page and selecting “Settings,” or directly visit the following link while signed in: https://github.com/settings/profile. In the left sidebar, click on “SSH and GPG keys.” Follow the instructions provided by GitHub to set up an SSH key for secure communication between your computer and GitHub. This will allow you to authenticate without entering your password each time you interact with GitHub via Git. Once the SSH key is set up, you can start using Git on your computer as described previously. You will be able to interact with your GitHub repositories by cloning them, pushing changes, and pulling updates.\nNote: For more details, follow steps at https://help.github.com/en/articles/generating-a-new-ssh-key-and-adding-it-to-the-ssh-agent and https://help.github.com/en/articles/adding-a-new-ssh-key-to-your-github-account\n3. Installing and setting up a GitHub Desktop Using GitHub Desktop eliminates the need to manually set up SSH keys, as the application handles authentication using your GitHub account credentials. It provides a more visual and user-friendly experience for working with Git and GitHub repositories.\nHere’s how you can use GitHub Desktop instead of setting up an SSH key:\nVisit the GitHub Desktop website. Install it. Open GitHub Desktop and click on the “Sign in to GitHub.com” button and enter your GitHub credentials to sign in. To clone an existing GitHub repository, click on “File” in the top menu, then “Clone Repository.” Select the repository you want to clone from the list, choose the destination folder on your computer, and click on the “Clone” button.\nOnce the repository is cloned, you can make changes to your files by editing, adding, or deleting them within the repository folder on your computer.\nGitHub Desktop will detect the changes automatically. You can review the changes, provide a summary of the changes made, and click on the “Commit” button to create a commit with the changes. You will also have the option to sync the changes to your GitHub repository using the “Push” button.\nGitHub Desktop provides an intuitive interface to manage branches, create new branches, pull changes from remote repositories, and perform various Git operations without using the command line.\nYou can also consider GitKraken as an alternative to GitHub Desktop.\nReferences: Happy Git and GitHub for the useR Install Git R packages",
    "description": "Installing and setting up Git, GitHub, and a Git GUI In this section we will install a distributed version-control system Git, register a new user at GitHub and connect them together.\n1. Installing and setting up Git Download Git installer from git website by clicking on “Download for Windows” or “Download for macOS” button, automatically detected for your machine. Open the downloaded file and follow the proposed steps Configure your Git to let it know who you are. Do it by opening a command prompt or Terminal window on your computer and set “Your Name” and “your_email@unil.ch{.email}” git config --global user.name \"Your Name\" git config --global user.email \"your_email@unil.ch\" Replace “Your Name” with your preferred name or the name you want to associate with your Git commits. Please also use your UNIL email address, so that we can exploit GitHub Student Developer Pack afterwards.",
    "tags": [],
    "title": "Git(hub) setup",
    "uri": "/en/tutorials/github/index.html"
  },
  {
    "breadcrumb": "Data and Code Management \u003e Tutorials",
    "content": "Installing R and RStudio 1. Installing R Install the latest version of R (4.3.1 as of September 5, 2023). R itself is similar to an engine and chassis of a car, that is a bare minimum so that you can start driving. You need to follow steps below:\nVisit https://cran.r-project.org and click on “Download R for …”, where … corresponds to your operating system. Depending on the operating system: For Mac: download “R-4.3.1-arm64.pkg”, open this file, and install R; For Windows: click on “base”, “Download R-4.3.1 for Windows”, and download the .exe file. Open it, and install R. Note: If you are a Mac user and you see similar to the following warning messages during the startup\nDuring startup - Warning messages: 1: Setting LC_CTYPE failed, using \"C\" 2: Setting LC_COLLATE failed, using \"C\" 3: Setting LC_TIME failed, using \"C\" 4: Setting LC_MESSAGES failed, using \"C\" 5: Setting LC_PAPER failed, using \"C\" [R.app GUI 1.50 (6126) x86_64-apple-darwin9.8.0] WARNING: You're using a non-UTF8 locale, therefore only ASCII characters will work. Please read R for Mac OS X FAQ (see Help) section 9 and adjust your system preferences accordingly. [History restored from /Users/nemo/.Rapp.history] you need to follow steps below:\nOpen Terminal Write or paste in: defaults write org.R-project.R force.LANG en_US.UTF-8 Close Terminal 2. Installing RStudio Caution: Install RStudio only once R has been installed and only in this order.\nRStudio is an integrated development environment for R. Following up our example of the car, RStudio is similar to additional parts, such as exterior, interior, air conditioner, etc. You can drive the vehicle without them, but life is much simpler and pleasant if they are present.\nWe will install the free version:\nVisit RStudio website. Click on the respective version of your operating system, this will start the downloading process. Open the file and install. Note: To improve the quality of the code, we will limit the length of lines to 80 symbols. To display the margin in RStudio sourse editor:\nOpen RStudio Go to Tools -\u003e Global Options… -\u003e Code -\u003e Display Click on “Show margin” Set “Margin column” to 80 Check yourself: Open RStudio application. In the console you will see something as follows:\nR version 4.3.1 (2023-06-16 ucrt) -- \"Beagle Scouts\" Copyright (C) 2023 The R Foundation for Statistical Computing Platform: x86_64-w64-mingw32/x64 (64-bit) R is free software and comes with ABSOLUTELY NO WARRANTY. You are welcome to redistribute it under certain conditions. Type 'license()' or 'licence()' for distribution details. R is a collaborative project with many contributors. Type 'contributors()' for more information and 'citation()' on how to cite R or R packages in publications. Type 'demo()' for some demos, 'help()' for on-line help, or 'help.start()' for an HTML browser interface to help. Type 'q()' to quit R. 3. Installing packages Note: Packages can be installed from both R and RStudio. The installed RStudio is not required.\nIn this course we will utilize a number of packages. If a package is published on CRAN, then the procedure of installing the package is straightforward:\nOpen RStudio In the console execute the following command: install.packages(\"package_name\"), where package_name is the name of the desired package (e.g., “ggplot2”). Several packages, however, would have only development version (or simply be not published on CRAN). Then, knowing the GitHub link to the repo, one could follow the steps below:\nInstall devtools package (if it has not yet been installed) as usual (as shown above). Type devtools::install_github(\"username/repo\") and hit the Enter/return key to execute the command in the console, where username is the username of the owner of the repo, and repo is the name of the repo. For homeworks you will use the following packages from CRAN: \"tidyverse\", \"devtools\", \"rmarkdown\", \"knitr\", \"shiny\", \"roxygen2\".\nNote: Before installing the \"devtools\" package, you will most certainly need to install building tools. For Windows, you need to install RTools. For Mac, you need to install XCode. Check this link for more details.\nNote: Packages should be installed only once. No needs to install them every time when you want to use them (it is the same as installing Skype every time you want to call your parents). That is why it is better to do it in console, not in source editor.\nCheck yourself: To check if a package was installed successfully, use \"name_of_package\" %in% rownames(installed.packages()).",
    "description": "Installing R and RStudio 1. Installing R Install the latest version of R (4.3.1 as of September 5, 2023). R itself is similar to an engine and chassis of a car, that is a bare minimum so that you can start driving. You need to follow steps below:\nVisit https://cran.r-project.org and click on “Download R for …”, where … corresponds to your operating system. Depending on the operating system: For Mac: download “R-4.3.1-arm64.pkg”, open this file, and install R; For Windows: click on “base”, “Download R-4.3.1 for Windows”, and download the .exe file. Open it, and install R. Note: If you are a Mac user and you see similar to the following warning messages during the startup",
    "tags": [],
    "title": "R/RStudio setup",
    "uri": "/en/tutorials/r_install/index.html"
  },
  {
    "breadcrumb": "Data and Code Management \u003e Syllabus",
    "content": "Location and Time Location: Anthropole/3032 Time: 9 - 12 (see the lectures schedule for more details) Course Websites Course website: https://dcm.samorso.ch/ Online textbook: https://smac-group.github.io/ds/ Discussion Forum: on slack Course Staff Instructor Name: Samuel Orso Email: Samuel.Orso@unil.ch Office: Anthropole/3090.1 Office Hours: Appointment may be made upon request. find me on GitHub Teaching Assistant Name: Timofei Shashkov Email: timofei.shashkov@unil.ch Office: Extranef/134 Office Hours: Appointment may be made upon request.",
    "description": "Location and Time Location: Anthropole/3032 Time: 9 - 12 (see the lectures schedule for more details) Course Websites Course website: https://dcm.samorso.ch/ Online textbook: https://smac-group.github.io/ds/ Discussion Forum: on slack Course Staff Instructor Name: Samuel Orso Email: Samuel.Orso@unil.ch Office: Anthropole/3090.1 Office Hours: Appointment may be made upon request. find me on GitHub Teaching Assistant Name: Timofei Shashkov Email: timofei.shashkov@unil.ch Office: Extranef/134 Office Hours: Appointment may be made upon request.",
    "tags": [],
    "title": "General information",
    "uri": "/en/syllabus/general/index.html"
  },
  {
    "breadcrumb": "Data and Code Management \u003e Tutorials",
    "content": "It’s like agreeing that we will all drive on the left or the right. A hallmark of civilization is following conventions that constrain your behavior a little, in the name of public safety. Jenny Bryan Introduction In this tutorial we will learn key aspects of making a good research project:\nreproducible portable self-contained In data science context, reproducibility means that the whole analysis can be recreated (or repeated) from the fresh start and raw data and get exactly the same results. It means, for instance, that if the analysis involves generating random numbers, then one has to set a seed (an initial state of a random generator) to obtain the same random split each time. Ideally, everyone should also have an access to data and software to replicate your analysis (it is not always the case, since data can be private).\nPortability means that regardless the operating system or a computer, for minimal given prerequisites, the project should work. For instance, if the project uses a particular package that works only on Windows, then it is not portable. The project is also not considered as portable, if it utilizes a particular computer settings, such as absolute paths instead of relative to your project folder (e.g., when reading the data or saving plots to files). Normally, you should be able to run the code on your collaborator’s machine without changing any lines in scripts.\nWe call a project self-contained, when you have everything you need at hand (i.e., in the folder of your project) and your porject does not affect anything it did not create. The project should not use a function, which you created in the other project five years ago – it is very likely that no one else has this function. Further, if you need, for instance, to save a processed data, then it should be saved separately, and not overwrite the raw data.\nWhy this is a big deal? First off, it gives more credibility to the research, because it can be verified and validated by a third party. Further, keeping the flow of analysis reproducible, portable and self-contained makes it easier to extend.\nThere are no clear boundaries between these three properties, they are very close in meaning, and often overlap. As a consequence, techniques and practice we consider further improve all of them, rather than focusing on a particular one.\nEven if it might look like a yet another git / RStudio tutorial, this is a list of my recommendations based on my own experience and various posts.\nProject folder structure The size of the project increases exponentially. A project started as a harmless code snippet can easily pile up into a huge snowball of over hundred files with unstructured folder tree. To avoid this, it is important do define the folder structure before stepping into analyses. Depending whether the project is a package or a case study, its skeleton differs significantly.\nThe folder structure of R packages is a subject to a regulation of community (CRAN and Bioconductor). It is well-defined and can be explored in R packages book, therefore, I skip it in this tutorial.\nIn contrast to R packages, there is no a single right folder structure for analysis projects. Below, I present a simple yet extensible folder structure for data analysis project, based on several references that cover this issue.\nname_of_project/ |- data | |- raw | |- processed |- figures |- reports |- results |- scripts | |- deprecated |- .gitignore |- name_of_project.Rproj |- README.md The parent folder that will contain all project’s subfolders should have the same name as your project. Pick a good one. Spending an extra 5 minutes will save you from regrets in the future. The name should be short, concise, written in lower-case, and not contain any special symbols. One can apply similar strategies as for naming packages.\nThe folder data typically contains two subfolders, namely, raw and processed. The content of raw directory is data files of any kind, such as .csv, SAS, Excel, text and database files, etc. The content of this folder is read only, so that no scripts should change the original files or create new ones. For this purpose the processed directory is used: all processed, cleaned, and tidied datasets are saved here. It is a good practice to save files in R format, rather than in .csv, since the former one is a more efficient way of storing data (both in terms of space and time of reading/writing). The preference is given to .rds files over .RData (see why in Content of R files section). Again, files should have representative names (merged_calls.rds vs dataset_1.rds). Note that it should be possible to regenerate those datasets from your scripts. In other words, if you remove all files from this folder, it must be possible to restore all of them by executing your scripts that use only the data from raw.\nThe folder figures is the place where you may store plots, diagrams and other figures. There is not much to say about it. Common extensions of such files are .eps, .png, .pdf, etc. Again, file names in the folder should be meaningful (the name img1.png does not represent anything).\nAll reports live in directory with the corresponding name reports. These reports can be of any formats, such as LaTeX, Markdown, R Markdown, Jupyter Notebooks, etc. Currently, more and more people prefer rich documents with text and executable code to LaTeX and friends.\nNot all output object of the analysis are data files. For example, you have calibrated and fitted your deep learning network to the data, which took about an hour. Of course, it would be painful to retrain the model each time you run the script, and you want to save this model. Then, it is reasonable to save it in results with .rmd extension.\nPerhaps the first by importance folder is scripts. There you keep all your R scripts and codes. That is the exact place to use prefix numbers, if files should be run in a particular order (see previous tutorial). If you have files in other scripted languages (e.g., Python), it better to locate them in this folder as well. There is also an important subfolder called depricated. Whenever you want to remove one or the other script, it is a good practice to move it to depricated at first iteration, and only then delete. The script you want to remove can contain functions or analysis used by other collaborators. Moving it firstly to depricated ensures that the file is not used by other collaborators.\nThere are three important files in the project folder: .gitignore, name_of_project.Rproj, and README.md. The file .gitignore lists files that won’t be added to Git system: LaTeX or C build artifacts, system files, very large files, or files generated for particular cases. Further, the name_of_project.Rproj contains options and meta-data of the project: encoding, the number of spaces used for indentation, whether or not to restore a workspace with launch, etc. The README.md briefly describes all high-level information about the project, like an abstract of a paper.\nThe proposed folder structure is far from being exhaustive. You might need to introduce other folders, such as paper (where .tex version of a paper lives), sources ( a place for your compiled code here, e.g. C++), references, presentations, NEWS.md, TODO.md, etc. At the same time, keeping an empty folders could be misleading, and it is better to remove them (unless you are planning to store anything in them in the future).\nSeveral R packages, namely ProjectTemplate, template, and template are dedicated to project structures. Also it is possible to construct a project tree by forking manuscriptPackage or sample-r-project repositories (repo for short). Using a package or forking a repo allow for automated structure generation, but at the same time introduce many redundant and unnecessary folders and files.\nFinally, some scientists believe that all R projects should be in a shape of a package. Indeed, one can store data in \\data, R scripts in \\R, documentation in \\man, and the paper \\vignette. The nice thing about it that anyone familiar with an R package structure can immediately grasp where each type of a file located. On the other hand, the structure of R packages is tailored to serve its purpose – make a coherent tool for data scientists and not to produce a data product: there is no distinction between functions definitions and applications, no proper place for reports, and finally there are no place for other script languages that you can use (e.g, Bash, Python, etc.).\nContent of R files While there are no rules how to organize your R code, there are several dos and don’ts that most of the time are not tough explicitly. I cover them below:\nDo not use the function install.packages() inside your scripts. You are not suppose to (re)install packages each time when you run your files. By default it is normally assumed that all packages that are used by a script are already installed.\nIf there are many of them to install, it is better to create a file configure.R, that will install all packages:\npkgs \u003c- c(\"ggplot2\", \"plyr\") install.packages(pkgs) The snippet above profits from the fact that install.packages() is a vectorized function. Anyway, most of the time, install.packages() is suppose to be called from the console, and not from the script.\nDo not use the function require(), unless it is a conscious choice. In contrast to library(), require() does not throw an error (only a warning) if the package is not installed.\nUse a character representation of the package name.\n# Good library(\"ggplot2\") # Bad library(ggplot2) Load only those packages that are actually used in the script. Load packages at the beginning of the script.\nDo not use rm(list = ls()) that erase your global environment. First, it could delete accidentally the precious heavy long-time-to-build object. Second, it gives an illusion of the fresh start of R.\nDo not use setwd(\"/Users/irudnyts/path/that/only/I/have\"). It is very unlikely that someone except you will have the same path to the project. Instead, use a package here and relative paths. The package here automatically recognizes the path to the project, and starts from there:\n# Good library(\"here\") cars \u003c- read.csv(file = here(\"data\", \"raw\", \"cars.csv\")) # Bad setwd(\"/Users/irudnyts/path/that/only/I/have/data/raw\") cars \u003c- read.csv(file = \"cars.csv\") If your script involves random generation, then set a seed by set.seed() function to get the same random split each time:\n# Good set.seed(1991) x \u003c- rnorm(100) # Bad x \u003c- rnorm(100) Do not repeat yourself (DRY). In R context it means the following: if the code repeated more than to times, you had better wrap it into a function.\n# Better fix_missing \u003c- function(x) { x[x == -99] \u003c- NA x } df[] \u003c- lapply(df, fix_missing) # Bad df$a[df$a == -99] \u003c- NA df$b[df$b == -99] \u003c- NA df$c[df$c == -99] \u003c- NA df$d[df$d == -99] \u003c- NA df$e[df$e == -99] \u003c- NA df$f[df$g == -99] \u003c- NA Separate function definitions from their applications.\nUse saveRDS() instead of save():\nsave() saves the objects and their names together in the same file; saveRDS() only saves the value of a single object (its name is dropped). load() loads the file saved by save(), and creates the objects with the saved names silently (if you happen to have objects in your current environment with the same names, these objects will be overridden); readRDS() only loads the value, and you have to assign the value to a variable. Yihui Xie Inizializing a new data analysis project Disclaimer: the procedure below can be done in different ways. This particular way is no better than the others, but from author opinion has the most logical flow.\nPrerequisites:\nInstalled and configured Git Installed R and RStudio Existing account in Github Steps:\nPick a good name (e.g., beer).\nIn RStudio create a project:\nNavigate to File -\u003e New project… Select New Directory Select New project (unless you are developing a package or a ShinyApp) Insert your picked name into Directory name Check Create a git repository This creates a folder with the name of the project, initialize a local git repo, generate an .Rproj file, and a .gitignore file.\nAdd a file structure as discussed in section, that is folder data (with raw and processed subfolders), figures, etc.\nCreate a README.md file.\nLaunch Terminal and navigate your working directory (of Terminal, not R) to your project folder by, for instance, cd /Users/irudnyts/Documents/projects/beer.\nRecord changes by git add --all and commit by git commit -m \"Create a folder structure of the project.\". Traditionally the message of the first commit is simple \"First commit.\", but I prefer to write something more conscious, like \"Create a folder structure of the project.\".\nNow all you changes are recoreded locally.\nNote also that Git does not record empty folders.\nCreate a new repo in GitHub (the same procedure holds for Bitbucket and Gitlab):\nFill in Repository name with the same name as your project. Fill in Description with one line that briefly explains the intent of the project and ends with full stop. (Check Private for homeworks). Hit Create repository. Connect your local repo to your Github repo by\ngit remote add origin git@github.com:irudnyts/beer.git git push -u origin master Refresh the page at your browser to ensure that changes appear at Github repo (do not freak out if you do not see all folders you have created, Git does not record empty folders).\nWorking with an existing data analysis project Pull changes introduced by your collaborators by git pull.\nModify your files. If you want to delete a script, first off, move it to \\depricated, and then remove it from there during the next iteration.\nAdd changes by git add --all and commit by git commit -m \"A concious commit message.\".\nPush changes by git push. Merge changes if needed.\nReferences R packages Project-oriented workflow save() vs saveRDS() Jupyter And R Markdown: Notebooks With R A sample R project structure sample-r-project repo Creating an analysis as a package and vignette Analyses as Packages Packages vs ProjectTemplate Organizing the project directory Designing projects Project Management With RStudio Folder Structure for Data Analysis Organizing files for data analysis A meaningful file structure for R projects Packaging data analytical work reproducibly using R (and friends) What’s in a Name? The Concepts and Language of Replication and Reproducibility Packaging Your Reproducible Analysis Tools for Reproducible Research Data Analysis and Visualization in R for Ecologists Stop the working directory insanity manuscriptPackage cboettig/template Pakillo/template A minimal Project Tree in R -ProjectTemplate Writing a paper with RStudio Reproducibility vs. Replicability: A Brief History of a Confused Terminology",
    "description": "It’s like agreeing that we will all drive on the left or the right. A hallmark of civilization is following conventions that constrain your behavior a little, in the name of public safety. Jenny Bryan Introduction In this tutorial we will learn key aspects of making a good research project:\nreproducible portable self-contained In data science context, reproducibility means that the whole analysis can be recreated (or repeated) from the fresh start and raw data and get exactly the same results. It means, for instance, that if the analysis involves generating random numbers, then one has to set a seed (an initial state of a random generator) to obtain the same random split each time. Ideally, everyone should also have an access to data and software to replicate your analysis (it is not always the case, since data can be private).",
    "tags": [],
    "title": "Project-oriented workflow",
    "uri": "/en/tutorials/workflow/index.html"
  },
  {
    "breadcrumb": "Data and Code Management \u003e Syllabus",
    "content": "This course introduces students to the essential tools and practices of modern data and code management, with a focus on business analytics applications.\nIt covers a broad spectrum of programming languages and platforms — including R, Python, SQL, GitHub, Power BI, and modern AI assistants (LLMs) — to help students design reproducible, collaborative, and impactful data workflows.\nTopics covered include: Reproducible research: Literate programming with RMarkdown and Jupyter Notebooks. Version control \u0026 collaboration: Best practices with GitHub. Programming foundations: Data structures, logical operators, control structures, and functions in R and Python. Databases \u0026 SQL: Querying, joining, and aggregating data, with integration into R and Python. Visualization \u0026 reporting: From exploratory analysis with ggplot2 and Python’s visualization libraries to interactive dashboards in Power BI. Software engineering for data science: Functions, testing, documentation, and package creation in R/Python. Web \u0026 data collection: Web scraping and APIs (using Rvest, BeautifulSoup, requests, etc.). Interactive applications: Development of web apps with Shiny and Streamlit. Large Language Models (LLMs): Leveraging AI assistants responsibly for code generation, documentation, and analytics. Communication \u0026 dissemination: Building reproducible websites, dashboards, and reports for sharing results. Learning outcomes By the end of the course, students will be able to:\nBuild and manage reproducible data pipelines across R, Python, and SQL. Develop and share interactive applications and dashboards. Apply best practices in coding, documentation, testing, and collaboration. Critically integrate AI-powered tools into their analytical workflow. Deliver professional, reproducible, and transparent projects from data collection to final presentation. No prior background in IT or programming is assumed, but students should bring curiosity and motivation to acquire hands-on programming skills that will serve them in their academic and professional careers.",
    "description": "This course introduces students to the essential tools and practices of modern data and code management, with a focus on business analytics applications.\nIt covers a broad spectrum of programming languages and platforms — including R, Python, SQL, GitHub, Power BI, and modern AI assistants (LLMs) — to help students design reproducible, collaborative, and impactful data workflows.\nTopics covered include: Reproducible research: Literate programming with RMarkdown and Jupyter Notebooks. Version control \u0026 collaboration: Best practices with GitHub. Programming foundations: Data structures, logical operators, control structures, and functions in R and Python. Databases \u0026 SQL: Querying, joining, and aggregating data, with integration into R and Python. Visualization \u0026 reporting: From exploratory analysis with ggplot2 and Python’s visualization libraries to interactive dashboards in Power BI. Software engineering for data science: Functions, testing, documentation, and package creation in R/Python. Web \u0026 data collection: Web scraping and APIs (using Rvest, BeautifulSoup, requests, etc.). Interactive applications: Development of web apps with Shiny and Streamlit. Large Language Models (LLMs): Leveraging AI assistants responsibly for code generation, documentation, and analytics. Communication \u0026 dissemination: Building reproducible websites, dashboards, and reports for sharing results. Learning outcomes By the end of the course, students will be able to:",
    "tags": [],
    "title": "Course description",
    "uri": "/en/syllabus/coursedescr/index.html"
  },
  {
    "breadcrumb": "Data and Code Management",
    "content": "Every Thursday from 9:00 to 12:00.\nThe ideal schedule is given below. It is subject to modifications.\nClass-related discussion and questions will be on Slack. Do not forget to register here.\nWarning Exceptionnally the Practical on week 6 will be on Friday 24 Oct.\nWeek Date Focus Session (3h) Homework 1 18 Sep 2025 Kick-off No class (Thesis \u0026 Career workshops) – 2 25 Sep 2025 Introduction \u0026 Tools (Lecture) - Course overview, expectations - Reproducibility in analytics - Tools: R, Python, SQL, GitHub, Markdown/Jupyter – 3 02 Oct 2025 Programming Foundations (Lecture) - Data structures (R \u0026 Python)- Control structures \u0026 functions- Good coding practices HW1 released (Programming foundations in R/Python) 4 09 Oct 2025 Practical 1 - R/Python coding drills (functions, loops, vectorization)- Git workflow practice HW1 discussed 5 16 Oct 2025 Data Management \u0026 Collection (Lecture) - Relational databases \u0026 SQL basics (SELECT, JOIN, GROUP BY)- Accessing SQL from R/Python- Web scraping (Rvest, BeautifulSoup)- APIs \u0026 JSON- Storage formats \u0026 best practices HW1 due HW2 released (SQL + scraping/API integration) 6 24 Oct 2025 Practical 2 - SQL queries + API/scraping mini-lab- Integrate results into R/Python HW2 discussed 7 30 Oct 2025 Software Engineering for Data Science (Lecture) - Object-oriented programming (R S3/S4, Python classes)- Functional programming- Package development- Testing \u0026 documentation HW2 due HW3 released (functions, package basics, testing) 8 06 Nov 2025 Practical 3 - Functions \u0026 package skeleton in R/Python- Documentation \u0026 testing basics HW3 discussed 9 13 Nov 2025 Modern Tools: BI \u0026 LLMs (Lecture) - Power BI for visualization \u0026 storytelling- Prompt engineering for LLMs- Using LLMs for code/documentation/tests- AI copilots for analytics HW3 due 10 20 Nov 2025 Practical 4 - LLM lab: generate SQL queries, R/Python snippets- Build BI dashboard (Power BI/Shiny/Streamlit) – 11 27 Nov 2025 Project Work I - Group project supervision- Focus on reproducibility, integration, and communication – 12 04 Dec 2025 Project Work II - Group sprint: project review- Hands-on project development – 13 11 Dec 2025 Project Work III - Project refinement- Preparing final deliverables – 14 18 Dec 2025 Project Presentations - Final presentations –",
    "description": "Every Thursday from 9:00 to 12:00.\nThe ideal schedule is given below. It is subject to modifications.\nClass-related discussion and questions will be on Slack. Do not forget to register here.\nWarning Exceptionnally the Practical on week 6 will be on Friday 24 Oct.\nWeek Date Focus Session (3h) Homework 1 18 Sep 2025 Kick-off No class (Thesis \u0026 Career workshops) – 2 25 Sep 2025 Introduction \u0026 Tools (Lecture) - Course overview, expectations - Reproducibility in analytics - Tools: R, Python, SQL, GitHub, Markdown/Jupyter – 3 02 Oct 2025 Programming Foundations (Lecture) - Data structures (R \u0026 Python)- Control structures \u0026 functions- Good coding practices HW1 released (Programming foundations in R/Python) 4 09 Oct 2025 Practical 1 - R/Python coding drills (functions, loops, vectorization)- Git workflow practice HW1 discussed 5 16 Oct 2025 Data Management \u0026 Collection (Lecture) - Relational databases \u0026 SQL basics (SELECT, JOIN, GROUP BY)- Accessing SQL from R/Python- Web scraping (Rvest, BeautifulSoup)- APIs \u0026 JSON- Storage formats \u0026 best practices HW1 due HW2 released (SQL + scraping/API integration) 6 24 Oct 2025 Practical 2 - SQL queries + API/scraping mini-lab- Integrate results into R/Python HW2 discussed 7 30 Oct 2025 Software Engineering for Data Science (Lecture) - Object-oriented programming (R S3/S4, Python classes)- Functional programming- Package development- Testing \u0026 documentation HW2 due HW3 released (functions, package basics, testing) 8 06 Nov 2025 Practical 3 - Functions \u0026 package skeleton in R/Python- Documentation \u0026 testing basics HW3 discussed 9 13 Nov 2025 Modern Tools: BI \u0026 LLMs (Lecture) - Power BI for visualization \u0026 storytelling- Prompt engineering for LLMs- Using LLMs for code/documentation/tests- AI copilots for analytics HW3 due 10 20 Nov 2025 Practical 4 - LLM lab: generate SQL queries, R/Python snippets- Build BI dashboard (Power BI/Shiny/Streamlit) – 11 27 Nov 2025 Project Work I - Group project supervision- Focus on reproducibility, integration, and communication – 12 04 Dec 2025 Project Work II - Group sprint: project review- Hands-on project development – 13 11 Dec 2025 Project Work III - Project refinement- Preparing final deliverables – 14 18 Dec 2025 Project Presentations - Final presentations –",
    "tags": [],
    "title": "Lectures",
    "uri": "/en/lectures/index.html"
  },
  {
    "breadcrumb": "Data and Code Management",
    "content": "Warning These tutorials are not updated. Last update, September 2024.\nThis section contains the tutorials on various topics, namely, coding style guide, basics of syntax, best practice, conventions, typical workflows, etc.",
    "description": "Warning These tutorials are not updated. Last update, September 2024.\nThis section contains the tutorials on various topics, namely, coding style guide, basics of syntax, best practice, conventions, typical workflows, etc.",
    "tags": [],
    "title": "Tutorials",
    "uri": "/en/tutorials/index.html"
  },
  {
    "breadcrumb": "Data and Code Management \u003e Syllabus",
    "content": "Learning outcomes for this course are assessed continuously through homework assignments and a semester-long group project.\nThere is no final exam. Instead, evaluation reflects your ability to apply the course material in practice and to collaborate effectively.\nGrading Breakdown Type Weight Homeworks (3 × 20%) 60% Group project 40% Participation \u0026 engagement Bonus Homeworks (60%) There will be three homework assignments (20% each), aligned with the major course blocks:\nHW1 (Weeks 3–5): Programming foundations in R and Python. HW2 (Weeks 5–7): SQL queries, web scraping, and API integration. HW3 (Weeks 7–9): Software engineering for data science (functions, packages, testing, documentation). Each homework is:\nReleased during a lecture (Weeks 3, 5, 7). Discussed the following week in the practical (Weeks 4, 6, 8). Due the week after (Weeks 5, 7, 9). Assignments are individual and must be submitted via GitHub Classroom.\nWarning Late submissions are penalized 1 point per 24 hours after the deadline, unless an extension is granted for documented reasons.\nGroup Project (40%) The semester project is the capstone assignment. Working in groups, you will design a complete data pipeline: from data collection and management (via SQL, APIs, web scraping) to analysis (R/Python), and finally to communication (dashboard, web app, or BI report).\nDeliverables must be hosted on GitHub and be fully reproducible.\nEvaluation criteria include:\nData pipeline \u0026 reproducibility (SQL integration, scripts, documentation) Code quality (clarity, modularity, version control, testing, literate programming) Application/visualization (Shiny/Streamlit app, Power BI dashboard, or similar) Final presentation (clarity, storytelling, reflection on tools and methods) Project timeline in the schedule:\nWeek 11: Project supervision \u0026 integration check Week 12: Mid-term project review (practical session) Week 13: Refinement \u0026 preparation of deliverables Week 14: Final presentations \u0026 peer review Warning The project requires significant teamwork, coordination, and planning. Start early!\nParticipation \u0026 Engagement (Bonus) Participation is not mandatory but can earn you a bonus on the final grade.\nStudents may volunteer to present exercise corrections during practical sessions.\nBonus points are awarded for clarity, correctness, and effort. Multiple contributions are possible, but the bonus is capped. The bonus can make a difference in borderline cases (e.g., raising a 3.5 to a 4.0). Summary Your grade will be determined by:\n60% three homeworks (Weeks 3–9) 40% semester-long group project (Weeks 11–14) Bonus for active engagement through exercise corrections This structure rewards both individual mastery (homeworks) and teamwork on applied projects, while encouraging active participation.",
    "description": "Learning outcomes for this course are assessed continuously through homework assignments and a semester-long group project.\nThere is no final exam. Instead, evaluation reflects your ability to apply the course material in practice and to collaborate effectively.\nGrading Breakdown Type Weight Homeworks (3 × 20%) 60% Group project 40% Participation \u0026 engagement Bonus Homeworks (60%) There will be three homework assignments (20% each), aligned with the major course blocks:\nHW1 (Weeks 3–5): Programming foundations in R and Python. HW2 (Weeks 5–7): SQL queries, web scraping, and API integration. HW3 (Weeks 7–9): Software engineering for data science (functions, packages, testing, documentation). Each homework is:",
    "tags": [],
    "title": "Grading",
    "uri": "/en/syllabus/grading/index.html"
  },
  {
    "breadcrumb": "Data and Code Management",
    "content": "Train yourself with additional exercises and their corrections.\nAttachments series/serie1 - correction.pdf (139 KB) series/serie1.pdf (121 KB)",
    "description": "Train yourself with additional exercises and their corrections.\nAttachments series/serie1 - correction.pdf (139 KB) series/serie1.pdf (121 KB)",
    "tags": [],
    "title": "Additional Exercises",
    "uri": "/en/exercises/index.html"
  },
  {
    "breadcrumb": "Data and Code Management \u003e Syllabus",
    "content": "Slack We will utilize Slack for discussing class-related matters and addressing inquiries. This platform is designed to facilitate collaborative learning, leveraging the combined wisdom of your peers and instructors. Please feel free to share questions with the entire class, especially if they pertain to course content. We strongly encourage you to seek clarification if you encounter difficulties grasping a concept and to offer assistance to your fellow classmates whenever possible by responding to their queries.\nEmail The preferred communication channel for most matters, whether public or private, is Slack.",
    "description": "Slack We will utilize Slack for discussing class-related matters and addressing inquiries. This platform is designed to facilitate collaborative learning, leveraging the combined wisdom of your peers and instructors. Please feel free to share questions with the entire class, especially if they pertain to course content. We strongly encourage you to seek clarification if you encounter difficulties grasping a concept and to offer assistance to your fellow classmates whenever possible by responding to their queries.",
    "tags": [],
    "title": "Communication",
    "uri": "/en/syllabus/communication/index.html"
  },
  {
    "breadcrumb": "Data and Code Management \u003e Syllabus",
    "content": "Laptops Please bring a laptop to class if possible. We encourage collaborative work, and ideally we aim for at least one laptop for every two students. Both homework assignments and the group project involve coding and teamwork, so having access to a laptop will make your learning experience smoother.\nIf you are considering purchasing a laptop, students from Swiss universities can benefit from preferential pricing through the Neptun Projekt or EPFL’s Poseidon.\nNote You do not need to buy a new laptop if you do not have one — sharing with classmates is fine.\nOperating Systems The course can be followed on MacOS, Windows, or Linux. All three operating systems are supported, and we provide installation guides for each. Linux remains a strong option for data science, but any modern laptop will work.\nTextbooks \u0026 Learning Resources There is no single required textbook. Instead, we rely on a combination of online books, documentation, and tutorials. All resources are freely available online.\nCore References R Programming R for Data Science by Garrett Grolemund \u0026 Hadley Wickham Advanced R by Hadley Wickham R Packages by Hadley Wickham \u0026 Jenny Bryan Python Programming Python Data Science Handbook by Jake VanderPlas Fluent Python (2nd Edition) by Luciano Ramalho SQL \u0026 Databases SQL for Data Science (UC Davis / Coursera) Mode SQL Tutorial Reproducibility \u0026 Collaboration Happy Git and GitHub for the useR by Jenny Bryan Pro Git Book by Scott Chacon \u0026 Ben Straub Business Intelligence \u0026 Visualization Power BI Documentation Mastering Shiny by Hadley Wickham Modern AI Tools Prompt Engineering Guide LangChain Documentation (for Python + LLM integration) Supplemental Resources ggplot2: Elegant Graphics for Data Analysis by Hadley Wickham Seamless R and C++ Integration with Rcpp by Dirk Eddelbuettel Engineering Production-Grade Shiny Apps by Colin Fay et al. Effective Python by Brett Slatkin We regroup additional references by topic in the resources page.\nSoftware All software used in this course is free and open source (or provided with academic licenses). We will work with:\nR (CRAN) with RStudio IDE Python (Anaconda Distribution or Miniconda) SQL (via SQLite, PostgreSQL, and connectors in R/Python) Git \u0026 GitHub for version control and collaboration Power BI Desktop (free academic license) Optional AI assistants (e.g. ChatGPT, Copilot) for coding and documentation support Detailed installation guides will be provided before the first practical session.",
    "description": "Laptops Please bring a laptop to class if possible. We encourage collaborative work, and ideally we aim for at least one laptop for every two students. Both homework assignments and the group project involve coding and teamwork, so having access to a laptop will make your learning experience smoother.\nIf you are considering purchasing a laptop, students from Swiss universities can benefit from preferential pricing through the Neptun Projekt or EPFL’s Poseidon.",
    "tags": [],
    "title": "Material",
    "uri": "/en/syllabus/material/index.html"
  },
  {
    "breadcrumb": "Data and Code Management",
    "content": "This page collects useful resources to support your learning in Data and Code Management: From Collection to Application.\nAll resources are freely available online.\nGetting started with R and RStudio The CRAN website — official source for R and its documentation. An Introduction to R — official tutorial covering basics of R programming and statistical analysis. RStudio IDE — the most widely used IDE for R. RStudio cheat sheets — concise guides for R packages and workflows. The tidyverse — collection of R packages for data manipulation, visualization, and analysis. Getting started with Python Python official documentation — language reference and tutorials. Python Data Science Handbook by Jake VanderPlas — free online guide to NumPy, pandas, matplotlib, scikit-learn. Real Python — tutorials and best practices for Python programming. Pandas documentation — data wrangling in Python. Matplotlib gallery — quick examples for Python plotting. SQL and Databases Mode SQL Tutorial — interactive beginner-to-advanced SQL lessons. SQLBolt — hands-on SQL exercises. PostgreSQL documentation — reference for one of the most popular open-source databases. SQLite documentation — lightweight relational database, easy to set up. Reproducibility and Collaboration Happy Git and GitHub for the useR by Jenny Bryan — beginner-friendly guide to version control. Pro Git Book — complete reference on Git. RMarkdown and Jupyter Notebooks — tools for literate programming. Quarto — next-generation publishing system for R, Python, and Julia. Business Intelligence \u0026 Visualization Power BI Documentation — tutorials and user guides from Microsoft. Mastering Shiny by Hadley Wickham — developing interactive web apps in R. Streamlit documentation — build web apps in Python for data science. ggplot2 book by Hadley Wickham — R visualization. Seaborn documentation — Python data visualization. Large Language Models (LLMs) and AI Tools Prompt Engineering Guide — strategies for effective LLM use. LangChain documentation — framework for building applications powered by LLMs in Python. OpenAI API documentation — reference for integrating LLMs into data workflows. Recommended Books Advanced R by Hadley Wickham. R for Data Science by Grolemund \u0026 Wickham. Fluent Python (2nd Edition) by Luciano Ramalho. Effective Python by Brett Slatkin. SQL for Data Scientists by Renee M. P. Teate. Miscellaneous Shiny — open-source R package for interactive apps. Streamlit — Python alternative for quick dashboards. Rcpp for Seamless R and C++ Integration — extend R with C++. Engineering Production-Grade Shiny Apps — best practices for robust Shiny development.",
    "description": "This page collects useful resources to support your learning in Data and Code Management: From Collection to Application.\nAll resources are freely available online.\nGetting started with R and RStudio The CRAN website — official source for R and its documentation. An Introduction to R — official tutorial covering basics of R programming and statistical analysis. RStudio IDE — the most widely used IDE for R. RStudio cheat sheets — concise guides for R packages and workflows. The tidyverse — collection of R packages for data manipulation, visualization, and analysis. Getting started with Python Python official documentation — language reference and tutorials. Python Data Science Handbook by Jake VanderPlas — free online guide to NumPy, pandas, matplotlib, scikit-learn. Real Python — tutorials and best practices for Python programming. Pandas documentation — data wrangling in Python. Matplotlib gallery — quick examples for Python plotting. SQL and Databases Mode SQL Tutorial — interactive beginner-to-advanced SQL lessons. SQLBolt — hands-on SQL exercises. PostgreSQL documentation — reference for one of the most popular open-source databases. SQLite documentation — lightweight relational database, easy to set up. Reproducibility and Collaboration Happy Git and GitHub for the useR by Jenny Bryan — beginner-friendly guide to version control. Pro Git Book — complete reference on Git. RMarkdown and Jupyter Notebooks — tools for literate programming. Quarto — next-generation publishing system for R, Python, and Julia. Business Intelligence \u0026 Visualization Power BI Documentation — tutorials and user guides from Microsoft. Mastering Shiny by Hadley Wickham — developing interactive web apps in R. Streamlit documentation — build web apps in Python for data science. ggplot2 book by Hadley Wickham — R visualization. Seaborn documentation — Python data visualization. Large Language Models (LLMs) and AI Tools Prompt Engineering Guide — strategies for effective LLM use. LangChain documentation — framework for building applications powered by LLMs in Python. OpenAI API documentation — reference for integrating LLMs into data workflows. Recommended Books Advanced R by Hadley Wickham. R for Data Science by Grolemund \u0026 Wickham. Fluent Python (2nd Edition) by Luciano Ramalho. Effective Python by Brett Slatkin. SQL for Data Scientists by Renee M. P. Teate. Miscellaneous Shiny — open-source R package for interactive apps. Streamlit — Python alternative for quick dashboards. Rcpp for Seamless R and C++ Integration — extend R with C++. Engineering Production-Grade Shiny Apps — best practices for robust Shiny development.",
    "tags": [],
    "title": "Resources",
    "uri": "/en/resources/index.html"
  },
  {
    "breadcrumb": "Data and Code Management \u003e Syllabus",
    "content": "",
    "description": "",
    "tags": [],
    "title": "Convinced?",
    "uri": "/en/syllabus/final/index.html"
  },
  {
    "breadcrumb": "",
    "content": "Welcome to the website of Data and Code Management: From Collection to Application, a course offered in the Master in Business Analytics at HEC Lausanne during the autumn semester 2025.\nThis course equips students with the foundational tools and best practices of modern data science, focusing on how to collect, manage, analyze, and share data in a reproducible and collaborative way.\nThroughout the semester, students will explore:\nProgramming in R and Python for data wrangling, visualization, and modeling. SQL and database management, essential for business analytics and large-scale data handling. Version control with GitHub, enabling collaboration and transparent workflows. Reproducible research using Quarto and Jupyter Notebook. Software engineering practices in data science (functions, testing, packages, documentation). Modern tools for communication and application, including Power BI dashboards, Shiny apps, and Python web apps. Large Language Models (LLMs) as assistants for coding, documentation, and analytics — used critically and responsibly. By the end of the course, students will be able to:\nBuild and document reproducible data pipelines across R, Python, and SQL. Create and share interactive applications (Shiny, Streamlit, Power BI). Manage projects and collaborate effectively using GitHub. Package their work as professional deliverables — from code libraries to dashboards. Critically integrate AI-powered tools into their analytical workflows. This course emphasizes hands-on learning and project-based application. Students will work in teams to develop a complete analytics project — from raw data collection to a polished, interactive presentation.\nWarning The first class is on Thursday 26 September. For the first class, we start at 8:30am. For the rest of the class, we will start at 9am.",
    "description": "This website provides the support for the class Data and Code Management: From Collection to Application given at the Faculty of Business and Economics (HEC Lausanne) of the University of Lausanne in Fall (currently 2025)",
    "tags": [],
    "title": "Data and Code Management",
    "uri": "/en/index.html"
  },
  {
    "breadcrumb": "Data and Code Management",
    "content": "",
    "description": "",
    "tags": [],
    "title": "Categories",
    "uri": "/en/categories/index.html"
  },
  {
    "breadcrumb": "Data and Code Management",
    "content": "",
    "description": "",
    "tags": [],
    "title": "Tags",
    "uri": "/en/tags/index.html"
  }
]
